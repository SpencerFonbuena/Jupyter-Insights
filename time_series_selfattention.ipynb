{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPswMXcsNyYRvnKU6ZnM1EW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SpencerFonbuena/MentorCruise/blob/main/time_series_selfattention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3AsZ-WvopTr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Below is an example of the attention, applied to my use case and with my understanding of each line. I've separated out each step on to different lines to keep things clear in the forward(). If a step has a \"q\" by it, it means there is a question in that step that I have\n",
        "\n",
        "### Comprehension Check and Thoughts\n",
        "1. qkv = self.qkv(data) # (64, 120, 1536) => We have now blended those original 512 features, into 1536 features.\n",
        "\n",
        "2. qkv = qkv.reshape(B, S, 3, -1) => Instead of having each token be described by one large vector, we have now 3 seperate representations of each token,\n",
        "    which will allow for a query, key, and value. Notice the dimension is back to the original number of features.\n",
        "\n",
        "3. qkv.reshape(B, S, 3, self.num_heads, -1) => As discussed in our session, instead of having one large vector that may be computationally expensive, we\n",
        "    have separated out that vector into 8 different representations of each query key and vector for each token. Doing it this way also provides intuition\n",
        "    into why the features dimension is features/heads. (512/8 = 64). This creates the same computational cost as the original.\n",
        "\n",
        "4. qkv.permute(2, 0, 3, 1, 4) => I've decided to do a toy example you can find below under the Markdown cell titled \"How does Permute Keep Track of Information\"\n",
        "\n",
        "5. qkv.reshape(3, B*self.num_heads, 120, -1).unbind(0) => I have a continued toy example under the Markdown cell \"What Effect does Combining Batches and Heads Have\"\n",
        "\n",
        "6. attn = (q * self.scale) @ k.transpose(-2, -1) => (512,120,120) (512 examples, 120 tokens, 120 scores) Each of the 512 scores is interpreted as the attention that\n",
        "    token should lend to each of the other 511 tokens. I have a further explanation under \"Why do we have to combine, and how does this help us with matmul\"\n",
        "\n",
        "7. attn = attn.softmax(dim=-1) => create a probability out of each of the attentions from the previous step. Will have the same dimensions\n",
        "\n",
        "8. x = (attn @ v) # (512, 120, 64) => Multiply each of the attention scores by the value of each token in each example, and sum. This will give each token its \"new identity\"\n",
        "\n",
        "Q9. x = x.view(B, self.num_heads, S, -1) => This is just re-separating the Batches from the num_heads. Q I'm not sure why it is a view instead of reshape. Q: Why did we use view here?\n",
        "    The articles I saw talk about how view will explicitely make a \"view\" of the tensor, and not a \"copy.\" Reshape on the other hand will make a view if possible, if not then copy\n",
        "    on contiguous memory space. Maybe using view here will save some space?\n",
        "\n",
        "10. x = x.permute(0, 2, 1, 3) => This is aligning the heads and the features together, so that we essentailly undo the reshaping that was done in step 3\n",
        "\n",
        "11. x = x.reshape(B, S, -1) => Again, the next step after attention is a fully connected linear layer. We combine them to prepare for that. it also is now the same shape\n",
        "    as it was when it came in. Only difference is that now it has learned a little more about itself throughout the attention operations.\n"
      ],
      "metadata": {
        "id": "yeVmJoeipSAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# my mock data input size\n",
        "data = torch.randn(64,120,512) # (Batch, Sequence, Features) => (batch, timestep, features)\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dim: int,\n",
        "                 num_heads: int = 8,) -> None:\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = nn.Linear(dim, dim * 3) # (64, 120, 512) @ (512, 512*3) | This creates 3x the number of features. 3x because there is 1 query + 1 key + 1 value = 3 representations of our original dataset\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim**-0.5 # The softmax is sensitive to large differences in values. Scaling provides some stability\n",
        "    def forward(self, data):\n",
        "        B, S, _ = data.shape\n",
        "        qkv = self.qkv(data) # (64, 120, 1536) => (64 examples, 120 tokens (in the form of timesteps), 1536 features)\n",
        "        qkv = qkv.reshape(B, S, 3, -1) # (64, 120, 3, 512) => (64 examples, 120 tokens, 3 attributes of each token (QKV), 512 features)\n",
        "        qkv = qkv.reshape(B, S, 3, self.num_heads, -1) # (64, 120, 3, 8, 64) => (64 examples, 120 tokens, 3 attributes of each token, 8 versions of each attribute, 64 features )\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, 64, 8, 120, 64)\n",
        "        q, k, v = qkv.reshape(3, B*self.num_heads, 120, -1).unbind(0) # each q, k, v has dimensions 3x(512,120,64) => 3 of (examples, tokens, features)\n",
        "\n",
        "        attn = (q * self.scale) @ k.transpose(-2, -1) # (512, 120, 120)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v) # (512, 120, 64)\n",
        "        x = x.view(B, self.num_heads, S, -1) # (64,8,120,64)\n",
        "        x = x.permute(0, 2, 1, 3) # (64, 120, 8, 64)\n",
        "        x = x.reshape(B, S, -1) # (64, 120, 512)\n",
        "\n",
        "\n",
        "#call Attention\n",
        "dim = data.shape[2]\n",
        "attention = Attention(dim)\n",
        "attention(data)"
      ],
      "metadata": {
        "id": "NAjVNEITpWBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQy5VRjbpebX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Does Permute Keep Track of Information"
      ],
      "metadata": {
        "id": "ErjbQyzlpkq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy = torch.arange(54).reshape(2,3,3,3) # (2 tokens, 3 qkv, 3 heads, 3 features)\n",
        "\n",
        "#raw dataset\n",
        "print(toy)\n",
        "\n",
        "#Representation of each element\n",
        "print(f'Token 1: \\n Query 1 & Head 1 {toy[0,0,0]}  \\n Query 1 & Head 2 {toy[0,0,1]}  \\n Query 1 & Head 3 {toy[0,0,2]}')\n",
        "print(f'Token 1: \\n Key 1 & Head 1 {toy[0,1,0]}  \\n Key 1 & Head 2 {toy[0,1,1]}  \\n Key 1 & Head 3 {toy[0,1,2]}')\n",
        "print(f'Token 1: \\n Value 1 & Head 1 {toy[0,2,0]}  \\n Value 1 & Head 2 {toy[0,2,1]}  \\n Value 1 & Head 3 {toy[0,2,2]}')\n",
        "\n",
        "print(f'\\n \\n Token 2: \\n Query 2 & Head 1 {toy[1,0,0]}  \\n Query 2 & Head 2 {toy[1,0,1]}  \\n Query 2 & Head 3 {toy[1,0,2]}')\n",
        "print(f'Token 2: \\n Key 2 & Head 1 {toy[1,0,0]}  \\n Key 2 & Head 2 {toy[1,0,1]}  \\n Key 2 & Head 3 {toy[1,0,2]}')\n",
        "print(f'Token 2: \\n Value 2 & Head 1 {toy[1,0,0]}  \\n Value 2 & Head 2 {toy[1,0,1]}  \\n Value 2 & Head 3 {toy[1,0,2]}')\n",
        "\n",
        "# Divided representation\n",
        "print(f'\\n\\nThe queries for token 1 are \\n{toy[0,0,:]} \\n {toy[1,0,:]}')\n",
        "print(f'\\n\\n The keys for token 1 are \\n{toy[0,1,:]} \\n {toy[1,1,:]}')\n",
        "print(f'\\n\\n The values for token 1 are \\n{toy[0,2,:]} \\n {toy[1,2,:]}')\n",
        "toy = torch.arange(54).reshape(2,3,3,3).permute(1,0,2,3) # As we can see, through permutation, we have simply grouped together the queries, keys, and values for each token.\n",
        "                                                         # The last two dimensions represent (heads, features) respectively.\n",
        "print(f'\\n \\n We can see, this permute simply grouped them together \\n{toy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEAQYXLVpl8g",
        "outputId": "9bee404b-3dc4-439e-f316-82cbcf3a8262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0,  1,  2],\n",
            "          [ 3,  4,  5],\n",
            "          [ 6,  7,  8]],\n",
            "\n",
            "         [[ 9, 10, 11],\n",
            "          [12, 13, 14],\n",
            "          [15, 16, 17]],\n",
            "\n",
            "         [[18, 19, 20],\n",
            "          [21, 22, 23],\n",
            "          [24, 25, 26]]],\n",
            "\n",
            "\n",
            "        [[[27, 28, 29],\n",
            "          [30, 31, 32],\n",
            "          [33, 34, 35]],\n",
            "\n",
            "         [[36, 37, 38],\n",
            "          [39, 40, 41],\n",
            "          [42, 43, 44]],\n",
            "\n",
            "         [[45, 46, 47],\n",
            "          [48, 49, 50],\n",
            "          [51, 52, 53]]]])\n",
            "Token 1: \n",
            " Query 1 & Head 1 tensor([0, 1, 2])  \n",
            " Query 1 & Head 2 tensor([3, 4, 5])  \n",
            " Query 1 & Head 3 tensor([6, 7, 8])\n",
            "Token 1: \n",
            " Key 1 & Head 1 tensor([ 9, 10, 11])  \n",
            " Key 1 & Head 2 tensor([12, 13, 14])  \n",
            " Key 1 & Head 3 tensor([15, 16, 17])\n",
            "Token 1: \n",
            " Value 1 & Head 1 tensor([18, 19, 20])  \n",
            " Value 1 & Head 2 tensor([21, 22, 23])  \n",
            " Value 1 & Head 3 tensor([24, 25, 26])\n",
            "\n",
            " \n",
            " Token 2: \n",
            " Query 2 & Head 1 tensor([27, 28, 29])  \n",
            " Query 2 & Head 2 tensor([30, 31, 32])  \n",
            " Query 2 & Head 3 tensor([33, 34, 35])\n",
            "Token 2: \n",
            " Key 2 & Head 1 tensor([27, 28, 29])  \n",
            " Key 2 & Head 2 tensor([30, 31, 32])  \n",
            " Key 2 & Head 3 tensor([33, 34, 35])\n",
            "Token 2: \n",
            " Value 2 & Head 1 tensor([27, 28, 29])  \n",
            " Value 2 & Head 2 tensor([30, 31, 32])  \n",
            " Value 2 & Head 3 tensor([33, 34, 35])\n",
            "\n",
            "\n",
            "The queries for token 1 are \n",
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]]) \n",
            " tensor([[27, 28, 29],\n",
            "        [30, 31, 32],\n",
            "        [33, 34, 35]])\n",
            "\n",
            "\n",
            " The keys for token 1 are \n",
            "tensor([[ 9, 10, 11],\n",
            "        [12, 13, 14],\n",
            "        [15, 16, 17]]) \n",
            " tensor([[36, 37, 38],\n",
            "        [39, 40, 41],\n",
            "        [42, 43, 44]])\n",
            "\n",
            "\n",
            " The values for token 1 are \n",
            "tensor([[18, 19, 20],\n",
            "        [21, 22, 23],\n",
            "        [24, 25, 26]]) \n",
            " tensor([[45, 46, 47],\n",
            "        [48, 49, 50],\n",
            "        [51, 52, 53]])\n",
            "\n",
            " \n",
            " We can see, this permute simply grouped them together \n",
            "tensor([[[[ 0,  1,  2],\n",
            "          [ 3,  4,  5],\n",
            "          [ 6,  7,  8]],\n",
            "\n",
            "         [[27, 28, 29],\n",
            "          [30, 31, 32],\n",
            "          [33, 34, 35]]],\n",
            "\n",
            "\n",
            "        [[[ 9, 10, 11],\n",
            "          [12, 13, 14],\n",
            "          [15, 16, 17]],\n",
            "\n",
            "         [[36, 37, 38],\n",
            "          [39, 40, 41],\n",
            "          [42, 43, 44]]],\n",
            "\n",
            "\n",
            "        [[[18, 19, 20],\n",
            "          [21, 22, 23],\n",
            "          [24, 25, 26]],\n",
            "\n",
            "         [[45, 46, 47],\n",
            "          [48, 49, 50],\n",
            "          [51, 52, 53]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Effect does Combining Batch and Heads have?"
      ],
      "metadata": {
        "id": "gmMHblX4pvts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy = toy.reshape(3, 6, 3) # we went from a permuted (3,2,3,3) to (3,6,3). We did this by multiplying our fictitous tokens and heads dimensions, which are equivalny to the batch and heads above\n",
        "print(f'Now we have our combined: Queries \\n {toy[0,:]}')\n",
        "print(f'Now we have our combined: Keys \\n {toy[1,:]}')\n",
        "print(f'Now we have our combined: Values \\n {toy[2,:]}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFZkgFnApwre",
        "outputId": "14d56017-a458-4e6a-90c5-834ac7bd9c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now we have our combined: Queries \n",
            " tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [27, 28, 29],\n",
            "        [30, 31, 32],\n",
            "        [33, 34, 35]])\n",
            "Now we have our combined: Keys \n",
            " tensor([[ 9, 10, 11],\n",
            "        [12, 13, 14],\n",
            "        [15, 16, 17],\n",
            "        [36, 37, 38],\n",
            "        [39, 40, 41],\n",
            "        [42, 43, 44]])\n",
            "Now we have our combined: Values \n",
            " tensor([[18, 19, 20],\n",
            "        [21, 22, 23],\n",
            "        [24, 25, 26],\n",
            "        [45, 46, 47],\n",
            "        [48, 49, 50],\n",
            "        [51, 52, 53]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why do we have to combine, and how does this help us with matmul"
      ],
      "metadata": {
        "id": "Yk-OFZdLp6na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It seems like combining batch and heads provides use the possibility to quickly matrix multiply each query with each key. As you can see, it is done correctly because\n",
        "#each row of query, is multiplied by each transposed column of key.\n",
        "\n",
        "q = torch.arange(54).reshape(3,6,3) # (3 examples, 6 tokens, 3 features)\n",
        "k = torch.arange(54).reshape(3,6,3).transpose(-2,-1) # (3 examples, 6 tokens, 3 features)\n",
        "print(f'Queries dot product \\n {q[0]} \\n {k[0]}')\n",
        "print(f'Keys dot product \\n {q[1]} \\n {k[1]}')\n",
        "print(f'Values dot product \\n {q[2]} \\n {k[2]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxnU6ObSp7Qm",
        "outputId": "d61c7f94-89db-4d11-fcfa-f1ff7efd59e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries dot product \n",
            " tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11],\n",
            "        [12, 13, 14],\n",
            "        [15, 16, 17]]) \n",
            " tensor([[ 0,  3,  6,  9, 12, 15],\n",
            "        [ 1,  4,  7, 10, 13, 16],\n",
            "        [ 2,  5,  8, 11, 14, 17]])\n",
            "Keys dot product \n",
            " tensor([[18, 19, 20],\n",
            "        [21, 22, 23],\n",
            "        [24, 25, 26],\n",
            "        [27, 28, 29],\n",
            "        [30, 31, 32],\n",
            "        [33, 34, 35]]) \n",
            " tensor([[18, 21, 24, 27, 30, 33],\n",
            "        [19, 22, 25, 28, 31, 34],\n",
            "        [20, 23, 26, 29, 32, 35]])\n",
            "Values dot product \n",
            " tensor([[36, 37, 38],\n",
            "        [39, 40, 41],\n",
            "        [42, 43, 44],\n",
            "        [45, 46, 47],\n",
            "        [48, 49, 50],\n",
            "        [51, 52, 53]]) \n",
            " tensor([[36, 39, 42, 45, 48, 51],\n",
            "        [37, 40, 43, 46, 49, 52],\n",
            "        [38, 41, 44, 47, 50, 53]])\n"
          ]
        }
      ]
    }
  ]
}