{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SpencerFonbuena/MentorCruise/blob/main/vision_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNCnnfkF2P_e"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l6kf5RJe2P_f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms as transforms\n",
        "from datetime import datetime as dt\n",
        "import tracemalloc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGf-E3-02P_h"
      },
      "source": [
        "## Resources Used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj9JrZfx2P_h"
      },
      "source": [
        "> #### Articles\n",
        ">> ##### An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [Paper](https://arxiv.org/pdf/2010.11929.pdf) | [Code](https://github.com/google-research/vision_transformer) | [Colab](https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax.ipynb)\n",
        "> #### Links\n",
        ">> ##### Hugging Face Implementation | [Code](https://huggingface.co/transformers/v4.5.1/_modules/transformers/models/vit/modeling_vit.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxjvX7Vb2P_h"
      },
      "source": [
        "## General Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4BwpIAv2P_h"
      },
      "source": [
        "> #### Questions\n",
        ">> ##### Why do we have to create image patches? Why not just feed the whole image in?\n",
        ">>> ##### Each token in the transformer will attend to every other token. In a typical 256x256 image used for vision tasks, that would mean 65536 tokens. With quadratic cost, it is not computationally feasible\n",
        "> ##### Mathematics\n",
        ">> ##### N = HW/P^2 = # of patches\n",
        ">> ##### P = Heigth and Width of the patch (P, P, C)\n",
        ">> ##### C = # of channels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v86vva92P_i"
      },
      "source": [
        "## Mathematics Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pne7B3x2P_i",
        "outputId": "99718a53-f7f4-4b5f-9ae2-6861f2cb768b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# [N = HW/P^2]\n",
        "pre_pro_img = (256,256,3) # (H, W, C)\n",
        "H, W, C = pre_pro_img\n",
        "P = 8 # => This is our patch size\n",
        "N = H*W/(P**2) # => 1024: This is the supposed number of patches according to the number of patches.\n",
        "# Assuming we take non-overlapping patches of an image, this would mean we have 256/8 x 256/8 patches. 256/8 = 32\n",
        "H, W, C = (32, 32, 3) # We now have 32x32 image patches. If we understand this correctly, 32*32 should equal 1024\n",
        "if N == H * W:\n",
        "    print('True')\n",
        "# [N = HW/P^2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWuGsxqk2P_i"
      },
      "source": [
        "## Possible methods of creating the patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0ShbXVwP2P_j"
      },
      "outputs": [],
      "source": [
        "# Reading in the image patches\n",
        "\n",
        "#I used a local image. For uploading reasons, I've created a random tensor\n",
        "'''rawimage = torchvision.io.read_image('/Users/spencerfonbuena/Desktop/Screenshot 2023-06-22 at 12.15.28 PM.png')\n",
        "PIL_img = T.ToPILImage()(rawimage)\n",
        "preimage = transforms.Resize(224)(PIL_img)\n",
        "image = transforms.ToTensor()(preimage)\n",
        "image = image.reshape(1,4,224,224)'''\n",
        "image = torch.randn(1,4,224,224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujg3M7_x2P_j"
      },
      "source": [
        "## My attempt at building one from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O84C_51b2P_j"
      },
      "source": [
        "> #### I attempted this before looking at an online implementation. I did change one thing after looking at other examples, which is outlined below\n",
        "> #### I checked with the real image to make sure that it does in fact create patches of it. The last permuting and reshaping to get the feature dimension doesn't make a whole lot of sense, but I saw that the other implementation had their dimensions in a certain way and I found a way to copy after the fact :)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnxrSJXg2P_j",
        "outputId": "9efacd7c-d9c5-43ed-e8e8-7f3a8afdba47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 196, 512])\n",
            "0:00:00.076021\n",
            "40099.00, 69516.00\n"
          ]
        }
      ],
      "source": [
        "#for fun comparison of supposed speed and memory allocation vs the better implementation.\n",
        "start = dt.now()\n",
        "tracemalloc.start()\n",
        "\n",
        "# Variable creation to generalize the code\n",
        "B, C, H, W = image.shape\n",
        "P = 16\n",
        "N = int(H/P)\n",
        "\n",
        "#I came to this solution using the toy example below\n",
        "reshape = torch.cat(image.reshape(B, C, H, N, P).unbind(2), dim=3).reshape(B, C, N, N, P**2).permute(0,1,3,2,4).reshape(B,C,N*N,P**2).permute(0,2,1,3).reshape(B,N*N,C*P**2)\n",
        "# frankly I'm not sure that the combining of the channels with the patch tokens even makes sense, but I saw the other example had (4,196,512) dimensions, and I decided to match it\n",
        "\n",
        "mlp = nn.Linear(C*P**2, 512)\n",
        "patch = mlp(reshape)\n",
        "print(patch.shape)\n",
        "# continued comparison metrics\n",
        "running_secs = (dt.now() - start)\n",
        "print(running_secs)\n",
        "current, peak =  tracemalloc.get_traced_memory()\n",
        "print(f\"{current:0.2f}, {peak:0.2f}\")\n",
        "tracemalloc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS497YKO2P_k"
      },
      "source": [
        "## Online Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJtPJ_E92P_k"
      },
      "source": [
        "> #### This image embedding, as I expressed on our chat I believe, doesn't just take pixels and box them, but rather creates embed_dim # of feature maps, each pulled from a 16x16 patch of the image, and feeds them into the transformer. It seems like in this way you get a rich representation, without having to attend to each pixel token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykEMuUVa2P_k",
        "outputId": "7a066eaf-8c68-4511-9ac1-6a1f134f47ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 196, 512])\n",
            "0:00:00.080698\n",
            "30371.00, 40639.00\n"
          ]
        }
      ],
      "source": [
        "#This is an example from ChatGPT. Similar examples can be found at the huggingface link\n",
        "\n",
        "# Example usage\n",
        "image_size = 224\n",
        "patch_size = 16\n",
        "in_channels = 4\n",
        "embed_dim = 512\n",
        "\n",
        "# Generate a random image tensor (these dimensions match the size of the actual image)\n",
        "batch_size = 4\n",
        "image = torch.randn(batch_size, in_channels, image_size, image_size)\n",
        "\n",
        "start = dt.now()\n",
        "tracemalloc.start()\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = image_size // patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x has shape (batch_size, channels, height, width)\n",
        "\n",
        "        # Reshape input to patches\n",
        "        patches = self.projection(x)  # Output shape: (batch_size, embed_dim, grid_size, grid_size)\n",
        "        patches = patches.flatten(2).transpose(1, 2)  # Output shape: (batch_size, grid_size^2, embed_dim)\n",
        "\n",
        "        return patches\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create an instance of the PatchEmbedding module\n",
        "patch_embedding = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
        "\n",
        "# Compute patch embeddings\n",
        "patches = patch_embedding(image)\n",
        "print(patches.shape)  # Output: (batch_size, grid_size^2, embed_dim)\n",
        "\n",
        "running_secs = (dt.now() - start)\n",
        "print(running_secs)\n",
        "current, peak =  tracemalloc.get_traced_memory()\n",
        "print(f\"{current:0.2f}, {peak:0.2f}\")\n",
        "tracemalloc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4V7hPxE2P_k"
      },
      "source": [
        "## Toy Example for Intuition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWBdlGsa2P_k"
      },
      "source": [
        "> #### toy.reshape(10,5,2)\n",
        ">> #### In a 10 by 10 image with patch size = 2, there will be 25 patches. If each position in the 2d matrix is numbered, then the first patch would consist of positions (0, 1, 10, 11), the second (2, 3, 12, 13)... Paying attention to the pattern, each flattened 2x2 matrix consists of (x, x+1, x+10, (x+1)+10). By reshaping it as (10,5,2), the first dimension's (dimension with 10 values in this case) first number will be +10 to the first value of the next dimension, which allows us to begin pairing values to get to our (0,1,10,11)\n",
        "> #### toy = torch.cat(toy.unbind(0), dim=-1)\n",
        ">> #### Now that we have our dimensions lined up, we unbind them so that we can place the desired dimensions together, and then concatenate those dimensions.\n",
        "> #### toy = toy.reshape(5,5,4)\n",
        ">> #### The output of our last operation had our desired (x, x+1, x+10, (x+1)+10), but it had the dimensions 5,20, essentially lining up multiple of our desired blocks. To get each on it's own row, we reshaped it. Now, we've got our desired patch size of 4 in the last dimension.\n",
        "> #### toy = toy.permute(1,0,2)\n",
        ">> #### I'm assuming we need the blocks in sequential order. This permute is to put them in order, going from top left to bottom right\n",
        "> #### toy = toy.reshape(25,4)\n",
        ">> #### Finally, we reshape it so we have our desired 25 patches of 2x2 flattened patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMCUzKhs2P_k",
        "outputId": "090dadd5-5d48-4300-9ff5-f78de13a022a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
            "        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
            "        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
            "        [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
            "        [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
            "        [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
            "        [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
            "        [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
            "        [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])\n",
            "tensor([[[ 0,  1],\n",
            "         [ 2,  3],\n",
            "         [ 4,  5],\n",
            "         [ 6,  7],\n",
            "         [ 8,  9]],\n",
            "\n",
            "        [[10, 11],\n",
            "         [12, 13],\n",
            "         [14, 15],\n",
            "         [16, 17],\n",
            "         [18, 19]],\n",
            "\n",
            "        [[20, 21],\n",
            "         [22, 23],\n",
            "         [24, 25],\n",
            "         [26, 27],\n",
            "         [28, 29]],\n",
            "\n",
            "        [[30, 31],\n",
            "         [32, 33],\n",
            "         [34, 35],\n",
            "         [36, 37],\n",
            "         [38, 39]],\n",
            "\n",
            "        [[40, 41],\n",
            "         [42, 43],\n",
            "         [44, 45],\n",
            "         [46, 47],\n",
            "         [48, 49]],\n",
            "\n",
            "        [[50, 51],\n",
            "         [52, 53],\n",
            "         [54, 55],\n",
            "         [56, 57],\n",
            "         [58, 59]],\n",
            "\n",
            "        [[60, 61],\n",
            "         [62, 63],\n",
            "         [64, 65],\n",
            "         [66, 67],\n",
            "         [68, 69]],\n",
            "\n",
            "        [[70, 71],\n",
            "         [72, 73],\n",
            "         [74, 75],\n",
            "         [76, 77],\n",
            "         [78, 79]],\n",
            "\n",
            "        [[80, 81],\n",
            "         [82, 83],\n",
            "         [84, 85],\n",
            "         [86, 87],\n",
            "         [88, 89]],\n",
            "\n",
            "        [[90, 91],\n",
            "         [92, 93],\n",
            "         [94, 95],\n",
            "         [96, 97],\n",
            "         [98, 99]]])\n",
            "tensor([[ 0,  1, 10, 11, 20, 21, 30, 31, 40, 41, 50, 51, 60, 61, 70, 71, 80, 81,\n",
            "         90, 91],\n",
            "        [ 2,  3, 12, 13, 22, 23, 32, 33, 42, 43, 52, 53, 62, 63, 72, 73, 82, 83,\n",
            "         92, 93],\n",
            "        [ 4,  5, 14, 15, 24, 25, 34, 35, 44, 45, 54, 55, 64, 65, 74, 75, 84, 85,\n",
            "         94, 95],\n",
            "        [ 6,  7, 16, 17, 26, 27, 36, 37, 46, 47, 56, 57, 66, 67, 76, 77, 86, 87,\n",
            "         96, 97],\n",
            "        [ 8,  9, 18, 19, 28, 29, 38, 39, 48, 49, 58, 59, 68, 69, 78, 79, 88, 89,\n",
            "         98, 99]])\n",
            "tensor([[[ 0,  1, 10, 11],\n",
            "         [20, 21, 30, 31],\n",
            "         [40, 41, 50, 51],\n",
            "         [60, 61, 70, 71],\n",
            "         [80, 81, 90, 91]],\n",
            "\n",
            "        [[ 2,  3, 12, 13],\n",
            "         [22, 23, 32, 33],\n",
            "         [42, 43, 52, 53],\n",
            "         [62, 63, 72, 73],\n",
            "         [82, 83, 92, 93]],\n",
            "\n",
            "        [[ 4,  5, 14, 15],\n",
            "         [24, 25, 34, 35],\n",
            "         [44, 45, 54, 55],\n",
            "         [64, 65, 74, 75],\n",
            "         [84, 85, 94, 95]],\n",
            "\n",
            "        [[ 6,  7, 16, 17],\n",
            "         [26, 27, 36, 37],\n",
            "         [46, 47, 56, 57],\n",
            "         [66, 67, 76, 77],\n",
            "         [86, 87, 96, 97]],\n",
            "\n",
            "        [[ 8,  9, 18, 19],\n",
            "         [28, 29, 38, 39],\n",
            "         [48, 49, 58, 59],\n",
            "         [68, 69, 78, 79],\n",
            "         [88, 89, 98, 99]]])\n",
            "tensor([[[ 0,  1, 10, 11],\n",
            "         [ 2,  3, 12, 13],\n",
            "         [ 4,  5, 14, 15],\n",
            "         [ 6,  7, 16, 17],\n",
            "         [ 8,  9, 18, 19]],\n",
            "\n",
            "        [[20, 21, 30, 31],\n",
            "         [22, 23, 32, 33],\n",
            "         [24, 25, 34, 35],\n",
            "         [26, 27, 36, 37],\n",
            "         [28, 29, 38, 39]],\n",
            "\n",
            "        [[40, 41, 50, 51],\n",
            "         [42, 43, 52, 53],\n",
            "         [44, 45, 54, 55],\n",
            "         [46, 47, 56, 57],\n",
            "         [48, 49, 58, 59]],\n",
            "\n",
            "        [[60, 61, 70, 71],\n",
            "         [62, 63, 72, 73],\n",
            "         [64, 65, 74, 75],\n",
            "         [66, 67, 76, 77],\n",
            "         [68, 69, 78, 79]],\n",
            "\n",
            "        [[80, 81, 90, 91],\n",
            "         [82, 83, 92, 93],\n",
            "         [84, 85, 94, 95],\n",
            "         [86, 87, 96, 97],\n",
            "         [88, 89, 98, 99]]])\n",
            "tensor([[ 0,  1, 10, 11],\n",
            "        [ 2,  3, 12, 13],\n",
            "        [ 4,  5, 14, 15],\n",
            "        [ 6,  7, 16, 17],\n",
            "        [ 8,  9, 18, 19],\n",
            "        [20, 21, 30, 31],\n",
            "        [22, 23, 32, 33],\n",
            "        [24, 25, 34, 35],\n",
            "        [26, 27, 36, 37],\n",
            "        [28, 29, 38, 39],\n",
            "        [40, 41, 50, 51],\n",
            "        [42, 43, 52, 53],\n",
            "        [44, 45, 54, 55],\n",
            "        [46, 47, 56, 57],\n",
            "        [48, 49, 58, 59],\n",
            "        [60, 61, 70, 71],\n",
            "        [62, 63, 72, 73],\n",
            "        [64, 65, 74, 75],\n",
            "        [66, 67, 76, 77],\n",
            "        [68, 69, 78, 79],\n",
            "        [80, 81, 90, 91],\n",
            "        [82, 83, 92, 93],\n",
            "        [84, 85, 94, 95],\n",
            "        [86, 87, 96, 97],\n",
            "        [88, 89, 98, 99]])\n"
          ]
        }
      ],
      "source": [
        "toy = torch.arange(100).reshape(10,10)\n",
        "# img = torch.cat(toy.reshape(10,5,2).unbind(0), dim=-1).reshape(5,5,4).permute(1,0,2).reshape(25,4) |=> On line example of the stretched out steps below.\n",
        "print(toy)\n",
        "toy = toy.reshape(10,5,2)\n",
        "print(toy)\n",
        "toy = torch.cat(toy.unbind(0), dim=-1)\n",
        "print(toy)\n",
        "toy = toy.reshape(5,5,4)\n",
        "print(toy)\n",
        "toy = toy.permute(1,0,2)\n",
        "print(toy)\n",
        "toy = toy.reshape(25,4)\n",
        "print(toy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pBEogoV42P_l",
        "outputId": "3297058a-b433-4326-ab08-db309a97c1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
            "        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
            "        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
            "        [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
            "        [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
            "        [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
            "        [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
            "        [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
            "        [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])\n",
            "tensor([[[ 0,  1],\n",
            "         [ 2,  3],\n",
            "         [ 4,  5],\n",
            "         [ 6,  7],\n",
            "         [ 8,  9]],\n",
            "\n",
            "        [[10, 11],\n",
            "         [12, 13],\n",
            "         [14, 15],\n",
            "         [16, 17],\n",
            "         [18, 19]],\n",
            "\n",
            "        [[20, 21],\n",
            "         [22, 23],\n",
            "         [24, 25],\n",
            "         [26, 27],\n",
            "         [28, 29]],\n",
            "\n",
            "        [[30, 31],\n",
            "         [32, 33],\n",
            "         [34, 35],\n",
            "         [36, 37],\n",
            "         [38, 39]],\n",
            "\n",
            "        [[40, 41],\n",
            "         [42, 43],\n",
            "         [44, 45],\n",
            "         [46, 47],\n",
            "         [48, 49]],\n",
            "\n",
            "        [[50, 51],\n",
            "         [52, 53],\n",
            "         [54, 55],\n",
            "         [56, 57],\n",
            "         [58, 59]],\n",
            "\n",
            "        [[60, 61],\n",
            "         [62, 63],\n",
            "         [64, 65],\n",
            "         [66, 67],\n",
            "         [68, 69]],\n",
            "\n",
            "        [[70, 71],\n",
            "         [72, 73],\n",
            "         [74, 75],\n",
            "         [76, 77],\n",
            "         [78, 79]],\n",
            "\n",
            "        [[80, 81],\n",
            "         [82, 83],\n",
            "         [84, 85],\n",
            "         [86, 87],\n",
            "         [88, 89]],\n",
            "\n",
            "        [[90, 91],\n",
            "         [92, 93],\n",
            "         [94, 95],\n",
            "         [96, 97],\n",
            "         [98, 99]]])\n",
            "tensor([[ 0,  1, 10, 11, 20, 21, 30, 31, 40, 41, 50, 51, 60, 61, 70, 71, 80, 81,\n",
            "         90, 91],\n",
            "        [ 2,  3, 12, 13, 22, 23, 32, 33, 42, 43, 52, 53, 62, 63, 72, 73, 82, 83,\n",
            "         92, 93],\n",
            "        [ 4,  5, 14, 15, 24, 25, 34, 35, 44, 45, 54, 55, 64, 65, 74, 75, 84, 85,\n",
            "         94, 95],\n",
            "        [ 6,  7, 16, 17, 26, 27, 36, 37, 46, 47, 56, 57, 66, 67, 76, 77, 86, 87,\n",
            "         96, 97],\n",
            "        [ 8,  9, 18, 19, 28, 29, 38, 39, 48, 49, 58, 59, 68, 69, 78, 79, 88, 89,\n",
            "         98, 99]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'print(toy)\\ntoy = toy.permute(1,0,2)\\nprint(toy)\\ntoy = toy.reshape(25,4)\\nprint(toy)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "toy = torch.arange(100).reshape(10,10)\n",
        "# img = torch.cat(toy.reshape(10,5,2).unbind(0), dim=-1).reshape(5,5,4).permute(1,0,2).reshape(25,4) |=> On line example of the stretched out steps below.\n",
        "print(toy)\n",
        "toy = toy.reshape(10,5,2)\n",
        "print(toy)\n",
        "toy = torch.cat(toy.unbind(0), dim=-1)\n",
        "print(toy)\n",
        "toy = toy.reshape(5,5,4)\n",
        "'''print(toy)\n",
        "toy = toy.permute(1,0,2)\n",
        "print(toy)\n",
        "toy = toy.reshape(25,4)\n",
        "print(toy)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsRWJTRz2P_n"
      },
      "source": [
        "## Visual of Patch Embedding Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKx5aCnG2P_n"
      },
      "source": [
        "![vit_figure.png](https://raw.githubusercontent.com/google-research/vision_transformer/main/vit_figure.png)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}