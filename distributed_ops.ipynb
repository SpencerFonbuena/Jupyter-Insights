{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SpencerFonbuena/MentorCruise/blob/main/distributed_ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYtP7bq2Im2O"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY3qb8G3Im2Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_DCUZ5UIm2R"
      },
      "source": [
        "# Model Parallelism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Dmu5UiWIm2R"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIK3YAhcIm2S"
      },
      "source": [
        "> #### Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | [Paper](https://arxiv.org/pdf/1909.08053.pdf)\n",
        "> #### Reducing Activation Recomputation in large Transformer Models | [Paper](https://docs.google.com/viewerng/viewer?url=https://arxiv.org/pdf/2205.05198.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fVHqzHeIm2S"
      },
      "source": [
        "## Tensor Parallelism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcCUyUYzIm2S"
      },
      "source": [
        "### 1. Feed Forward With Sync Point\n",
        "> As shown in the Megatron-LM, one way to split the feed forward on to different processors is to cut the input column wise, and the weight matrix row wise. An explicit example is given below. The only issue, is that it requires a sync point before the Gelu. The sync point is represented by the additon on the 2 gpu example. The reason we have to sync is because GeLU(AB+A2B2) != GeLU(AB)+GeLU(A2B2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxjWcWdaIm2S",
        "outputId": "a96ffd5e-0500-4a5a-e9dd-5b398848ba25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 50,  60],\n",
              "        [114, 140],\n",
              "        [178, 220]])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Single GPU (+1 at the end to 1 index, instead of 0 index)\n",
        "A = torch.arange(12).reshape(3,4) + 1\n",
        "B = torch.arange(8).reshape(4,2) + 1\n",
        "z = torch.matmul(A, B)\n",
        "z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sbQ0L0uIm2T",
        "outputId": "e890cc69-bd11-408c-cab6-cb937539b9fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 50,  60],\n",
              "        [114, 140],\n",
              "        [178, 220]])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#2 GPUs\n",
        "\n",
        "    # [Splitting A columnwise means (3,4) turns into 2x(3,2)]\n",
        "A = torch.tensor([[1,2],\n",
        "                  [5,6],\n",
        "                  [9,10]])\n",
        "A2 = torch.tensor([[3,4],\n",
        "                   [7,8],\n",
        "                   [11,12]])\n",
        "\n",
        "B = torch.tensor([[1,2],\n",
        "                  [3,4]])\n",
        "B2 = torch.tensor([[5,6],\n",
        "                   [7,8]])\n",
        "\n",
        "#   [GPU 0]               [GPU1]\n",
        "x = torch.matmul(A2,B2)\n",
        "y = torch.matmul(A,B)\n",
        "x+y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL5P9BktIm2T"
      },
      "source": [
        "### 2. Feed Forward Without Sync Point\n",
        "> An alternative is to split the weight matrix column wise for the first GEMM, which gets rid of our sync point (I believe it is because we simply have to concatenate the matrices, not add them). For the second GEMM, we can split it row wise as we did before, and then reduce it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpRiYuP5Im2U",
        "outputId": "52146208-1455-4d4b-95c7-abff4fa253fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 50,  60],\n",
              "        [114, 140],\n",
              "        [178, 220]])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Single GPU (+1 at the end to 1 index, instead of 0 index)\n",
        "A = torch.arange(12).reshape(3,4) + 1\n",
        "B = torch.arange(8).reshape(4,2) + 1\n",
        "torch.matmul(A, B)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaEOBw8tIm2U",
        "outputId": "f8d1472e-70ce-4500-ec5d-fc6914370e71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 50,  60],\n",
              "        [114, 140],\n",
              "        [178, 220]])"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2 GPU training\n",
        "# [B & B2 represent the weight matrix split down the column]\n",
        "\n",
        "# GPU 1\n",
        "B = torch.tensor([[1],[3],[5],[7]])\n",
        "# GPU 2\n",
        "B2 = torch.tensor([[2],[4],[6],[8]])\n",
        "\n",
        "# GPU 1\n",
        "x = torch.matmul(A, B)\n",
        "# GPU 2\n",
        "y = torch.matmul(A, B2)\n",
        "torch.cat([x, y], dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1qvN4g5Im2U"
      },
      "source": [
        "### 3. MHA\n",
        "> #### [1]In the paper, it looks like the parallelization happens once we have our attention heads. For that reason, I waited until we unbound the q, k, and v. Once they are unbound, I cut apart the heads into the number of GPUs we have access to.\n",
        "> #### [2] In order to parallelize the heads, we have to split them up into different variables to place on to different processors.\n",
        "> #### [General] It was at first confusing to me how it was ok to split up the MHA. But it helps to look at the dimensions of each q, k, v, and look at what they mean:\n",
        ">>The dims of q,k,v = (512,120,64) => (heads (for each example), tokens, features). When we do matmul(q, k.transpose) we are multiplying each of the last two dimensions (120,64)(64,120), 512 different times. Each one of those 512 matrices are distinct. that means that the matrix multiply of the 1st head, has nothing to do with the matrix multiply of the 2nd head. That way, when we split the MHA on that dimension, there is no information corruption\n",
        "> #### [3] We then have to concatenate them all back together, for the linear layer that extracts the information from the multi-headed attention layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn3Pk9tkIm2U"
      },
      "outputs": [],
      "source": [
        "# my mock data input size\n",
        "data = torch.randn(64,120,512) # (Batch, Sequence, Features) => (batch, timestep, features)\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dim: int,\n",
        "                 num_heads: int = 8,\n",
        "                 num_gpus: int = 2) -> None:\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = nn.Linear(dim, dim * 3) # (64, 120, 512) @ (512, 512*3) | This creates 3x the number of features. 3x because there is 1 query + 1 key + 1 value = 3 representations of our original dataset\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim**-0.5\n",
        "        self.num_gpus = num_gpus\n",
        "\n",
        "    def forward(self, data):\n",
        "        B, S, _ = data.shape\n",
        "        qkv = self.qkv(data) # (64, 120, 1536) => (64 examples, 120 tokens (in the form of timesteps), 1536 features)\n",
        "        qkv = qkv.reshape(B, S, 3, -1) # (64, 120, 3, 512) => (64 examples, 120 tokens, 3 attributes of each token (QKV), 512 features)\n",
        "        qkv = qkv.reshape(B, S, 3, self.num_heads, -1) # (64, 120, 3, 8, 64) => (64 examples, 120 tokens, 3 attributes of each token, 8 versions of each attribute, 64 features )\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, 64, 8, 120, 64)\n",
        "        q, k, v = qkv.reshape(3, B*self.num_heads, 120, -1).unbind(0) # each q, k, v has dimensions 3x(512,120,64) => 3 of (examples, tokens, features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # [Add this operation for tensor parallelism]\n",
        "\n",
        "            # [1] [I have reshaped the Qs, Ks, and Vs, in order to have the number of GPUs in the first\n",
        "            #  dimension. This allows us to unbind them, and therefore split up each head onto a different GPU]\n",
        "        qs = q.reshape(self.num_gpus, int(B*self.num_heads / self.num_gpus), S, -1).unbind(0)\n",
        "        ks = q.reshape(self.num_gpus, int(B*self.num_heads / self.num_gpus), S, -1).unbind(0)\n",
        "        vs = q.reshape(self.num_gpus, int(B*self.num_heads / self.num_gpus), S, -1).unbind(0)\n",
        "\n",
        "            # [2] [This is my attempt to assign each one of those different chunks of the model onto different variables,\n",
        "            #  This will hopefully allow us to then place them on different processors and comput in parallel]\n",
        "        for i in qs:\n",
        "            qs[i].to(f'cuda:{i}')\n",
        "        for i in ks:\n",
        "            ks[i].to(f'cuda:{i}')\n",
        "        for i in vs:\n",
        "            ks[i].to(f'cuda:{i}')\n",
        "\n",
        "            # [At this point, we have the data on different processors, now we would need to copy each operation\n",
        "            #  on the individual GPUs and run them. What I'm not sure of is how to get them to go at the same time.]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        attn = (q * self.scale) @ k.transpose(-2, -1) # (512, 120, 120)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v) # (512, 120, 64)\n",
        "        x = x.view(B, self.num_heads, S, -1) # (64,8,120,64)\n",
        "        x = x.permute(0, 2, 1, 3) # (64, 120, 8, 64)\n",
        "        x = x.reshape(B, S, -1) # (64, 120, 512)\n",
        "\n",
        "\n",
        "\n",
        "        #[3] [We then would have to do something like the following, assuming that we did each computation on the respective GPU, to concatenate the results and project it into a MLP]\n",
        "            # [For demonstration, I assumed we only have 2 GPUs, as opposed to above where I tried to generalize the case]\n",
        "        x = self.proj(torch.cat[gpu1, gpu2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6t3I9LPIm2V"
      },
      "source": [
        "## Sequence Parallelism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ll8mr37Im2V"
      },
      "source": [
        "### 1. Layer Norm\n",
        "> #### [1]Below is a toy example to demonstrate which dimension layernorm affects. As we can see, it normalizes the channel dimension, and is unnafected by other sequence tokens. (If it were affected, there wouldn't be 0's, as each column input is different)\n",
        "> #### [2]The fact that sequence tokens are unnafected by each other in this operation means that we can parallelize them. We also should be able to combine the sequence and batch dimension to separate the sequence more effectively. We compare the two outcomes from 1 and 2 and see they are the same, which means we didn't corrupt the data through our parallelization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahps55XiIm2V",
        "outputId": "4186d1ea-4623-4040-c67a-dbe5bf790e0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.2673, -1.0690,  1.3363],\n",
              "         [ 0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#[1]\n",
        "\n",
        "batch, sentencelength, embeddingdim = 2,2,3\n",
        "embedding = torch.arange(12).reshape(batch, sentencelength, embeddingdim).float()\n",
        "embedding = torch.tensor([[[2,1,4],\n",
        "                           [10,10,10]],\n",
        "                          [[100,100,100],\n",
        "                           [1000,1000,1000]]]).float()\n",
        "layernorm = nn.LayerNorm(embeddingdim)\n",
        "\n",
        "layernorm(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JvdEkmOIm2V",
        "outputId": "3ab0b3e2-b67c-44c9-ebc4-353de4e1c3b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.2673, -1.0690,  1.3363],\n",
              "         [ 0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<ReshapeAliasBackward0>)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#[2]\n",
        "embedding = embedding.reshape(4,3)\n",
        "embedding1 = embedding[:2,:]\n",
        "embedding2 = embedding[2:,:]\n",
        "\n",
        "layernorm1 = nn.LayerNorm(embeddingdim)\n",
        "layernorm2 = nn.LayerNorm(embeddingdim)\n",
        "\n",
        "layernorm1 = layernorm(embedding1)\n",
        "layernorm2 = layernorm(embedding2)\n",
        "\n",
        "output = torch.cat([layernorm1, layernorm2]).reshape(2,2,3)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWwSpnSpIm2V"
      },
      "source": [
        "### 2. Dropout\n",
        "> #### Dropout is a per element operation. This means that it realistically could be implemented with either sequence, or tensor parallelism. In the paper however, it is done in the sequence parallelism, probably because that is where we are coming from I imagine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIkcgrqmIm2W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}